---
format: markdown
categories: a-cause
...

<!-- keywords:
archives archive archival internet archive hoarding
saving things preserving preservation
-->

# Places to look

- gwern on [link rot](http://www.gwern.net/Archiving%20URLs), and on [Long Content](http://www.gwern.net/About#long-content)

- Some notes from Issa Rice's Quora blog on [data archiving](https://www.quora.com/Issa-Rice/Data-Archiving)

- Wikipedia [has an article](https://en.wikipedia.org/wiki/Digital_preservation)

- [what would it take for historians to be able to share archival material?](http://www.andrewjberger.net/wordpress/2012/08/29/what-would-it-take-for-historians-to-be-able-to-share-archival-material/)

- <https://www.youtube.com/watch?v=NdZxI3nFVJs&feature=youtu.be>

- <http://arxiv.org/abs/1212.6177>

- [Archiveteam](http://www.archiveteam.org/index.php?title=Main_Page)

# Importance

FIXME

# Tractability

There are several levels of tractability with regard to digital preservation:

- The tractability of the ultimate goal of archiving all of the web. This impossible to do fully, but at least a large amount of the web can be (and has been) archived. Some things to consider there are:
    - How difficult is it to enumerate all sites that need to be archived?
- The tractability of archiving individual sites. Static pages that just have HTML/CSS are the easiest, but those that have more complicated media (e.g. Flash, videos) are more difficult to archive, as are sites that use AJAX/Javascript to dynamically load content (e.g. sites that employ infinite scrolling). In addition, some sites use IP blocking to prevent people from archiving the site.
    - Figuring out the scope of websites seems tractable. Even for sites like [the Internet Archive](http://www.archiveteam.org/index.php?title=INTERNETARCHIVE.BAK), there seems to be concrete figures on what it would take to fully backup the site.

# Neglectedness

FIXME
