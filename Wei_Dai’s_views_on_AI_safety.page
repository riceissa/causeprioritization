---
title: Wei Dai’s views on AI safety
format: markdown
categories: AI_safety
...

The following table summarizes Wei Dai's views on topics in AI safety.

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines|Wei talks a lot about concerns of shortening AI timelines (e.g. by continuing his decision theory work), but I haven't really seen him give an estimate of when he expects human-level AI to arrive.|
|Value of decision theory work|see [here](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ycj). "For example I've mostly stopped working on decision theory because it seems to help UFAI as much as FAI." from [this comment](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ya3)|
|Value of highly reliable agent design work|see comments like [this one](http://lesswrong.com/lw/g93/evaluating_the_feasibility_of_sis_plan/89go), [this one](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5yfg), [this one](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ycj), [this one](http://lesswrong.com/lw/6mi/some_thoughts_on_singularity_strategies/4its)|
|Difficulty of AI alignment|Seems to be very pessimistic.|
|Shape of takeoff/discontinuities in progress|"I guess I would describe my overall view as being around 50/50 uncertain about whether the Singularity will be Yudkowsky-style (fast local FOOM) or Hanson-style (slower distributed FOOM)."[^foom]|
|Type of AI safety work most endorsed|He has endorsed (1) strategy research; (2) intelligence enhancement/amplification;[^ia_comment][^singularity_strategies] (3) "pushing for a government to try to take an insurmountable tech lead via large scale intelligence enhancement"; (4) philosophy research on topics like consciousness, normative ethics, and meta-ethics, which he thinks will not shorten AI timelines; and (5) advocacy/outreach.[^policy_alternatives] He also seems less worried about whole-brain emulation compared to de novo AGI.[^wbe_less_danger] See also [this comment](http://lesswrong.com/lw/2jd/open_thread_august_2010/2cv2). And [this comment](https://www.greaterwrong.com/posts/vrnhfGuYTww3fKhAM/three-approaches-to-friendliness#comment-nBGMxH4bEykD2wvpd).|
|How "prosaic" AI will be|He hasn't said anything about this as far as I can tell.|
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Difficulty of philosophy|Philosophy is hard. Wei has discussed this in many places. See [here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o19) for one discussion. See [here](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/dx46) for a recent comment. I'm not aware of a single comprehensive overview of his views on the difficulty of philosophy.|
|How well we need to understand philosophy before building AGI|We need to understand philosophy well. See some of the discussions with Paul Christiano. See also threads like [this one](http://lesswrong.com/lw/ua/the_level_above_mine/8b8u). "I think we need to solve metaethics and metaphilosophy first, otherwise how do we know that any proposed solution to normative ethics is actually correct?"[^metaethics_first] "I guess there is a spectrum of concern over philosophical problems involved in building an FAI/AGI, and I'm on the far end of the that spectrum. I think most people building AGI mainly want short term benefits like profits or academic fame, and do not care as much about the far reaches of time and space, in which case they'd naturally focus more on the immediate engineering issues."[^philosophy_spectrum]|
|How much alignment work is possible early on|"My model of FAI development says that you have to get most of the way to being able to build an AGI just to be able to *start* working on many Friendliness-specific problems, and solving those problems would take a long time relative to finishing rest of the AGI capability work."[^fai_dev] In a slow-FOOM scenario, it is difficult to work ahead because new AI architectures will continue to be developed.[^ahead_of_time]|
|Hardware/computing overhang|I haven't seen him talk about this at all.|
|Relationship between ability of AI alignment team and the probability of good outcomes|He has a chart in [this comment](http://lesswrong.com/lw/8c3/qa_with_new_executive_director_of_singularity/593u).|

# See also

- [List of discussions between Paul Christiano and Wei Dai]()

# External links

- [Timeline of Wei Dai publications](https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications)

[^fai_dev]: [“Wei\_Dai comments on How does MIRI Know it Has a Medium Probability of Success?”](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9j6s) LessWrong. Retrieved March 8, 2018.

[^metaethics_first]: [“Wei\_Dai comments on AALWA: Ask any LessWronger anything”](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/ap84). LessWrong. Retrieved March 8, 2018.

[^philosophy_spectrum]: [“Wei\_Dai comments on AALWA: Ask any LessWronger anything”](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/aq5j). LessWrong. Retrieved March 8, 2018.

[^policy_alternatives]: [“Wei\_Dai comments on How does MIRI Know it Has a Medium Probability of Success?”](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9j8n). LessWrong. Retrieved March 8, 2018.

[^ia_comment]: [“Wei\_Dai comments on Cynical explanations of FAI critics (including myself)”](http://lesswrong.com/lw/e2k/cynical_explanations_of_fai_critics_including/7786). LessWrong. Retrieved March 8, 2018.

[^singularity_strategies]: Wei Dai (July 13, 2011). [“Some Thoughts on Singularity Strategies”](http://lesswrong.com/lw/6mi/some_thoughts_on_singularity_strategies/). LessWrong. Retrieved March 8, 2018.

[^foom]: [“Wei\_Dai comments on How can we ensure that a Friendly AI team will be sane enough?”](http://lesswrong.com/lw/cfc/how_can_we_ensure_that_a_friendly_ai_team_will_be/9oo1). LessWrong. Retrieved March 8, 2018.

[^ahead_of_time]: Wei Dai (August 31, 2013). [“Wei\_Dai comments on Outside View(s) and MIRI's FAI Endgame”](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o74). *LessWrong*. Retrieved March 8, 2018.

    > The way I model AGI development in a slow-FOOM scenario is that AGI capability will come in spurts along with changing architectures, and it's hard to do AI safety work "ahead of time" because of dependencies on AI architecture. So each time there is a big AGI capability development, you'll be forced to spend time to develop new AI safety tech for that capability/architecture, while others will not wait to deploy it. Even a small delay can lead to a large loss since AIs can be easily copied and more capable but uncontrolled AIs would quickly take over economic niches occupied by existing humans and controlled AIs.

[^wbe_less_danger]: Wei Dai (August 28, 2012). [“Wei\_Dai comments on Stupid Questions Open Thread Round 4”](http://lesswrong.com/lw/e97/stupid_questions_open_thread_round_4/7a81). *LessWrong*. Retrieved March 8, 2018.
