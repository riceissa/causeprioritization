---
title: Wei Dai’s views on AI safety
format: markdown
categories: AI_safety
...

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines|Wei talks a lot about concerns of shortening AI timelines (e.g. by continuing his decision theory work), but I haven't really seen him give an estimate of when he expects human-level AI to arrive.|
|Value of decision theory work|see [here](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ycj). "For example I've mostly stopped working on decision theory because it seems to help UFAI as much as FAI." from [this comment](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ya3)|
|Value of highly reliable agent design work||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed|I think somewhere he talked about how working on intelligence amplification to solve philosophical problems is more promising. See e.g. [here](http://lesswrong.com/lw/e2k/cynical_explanations_of_fai_critics_including/7786), [this post](http://lesswrong.com/lw/6mi/some_thoughts_on_singularity_strategies/ "Wei Dai (July 13, 2011). “Some Thoughts on Singularity Strategies”. LessWrong.").|
|How "prosaic" AI will be||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Difficulty of philosophy|Philosophy is hard. Wei has discussed this in many places. See [here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o19) for one discussion. See [here](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/dx46) for a recent comment. I'm not aware of a single comprehensive overview of his views on the difficulty of philosophy.|
|How well we need to understand philosophy before building AGI|We need to understand philosophy well. See some of the discussions with Paul Christiano. See also threads like [this one](http://lesswrong.com/lw/ua/the_level_above_mine/8b8u). "I think we need to solve metaethics and metaphilosophy first, otherwise how do we know that any proposed solution to normative ethics is actually correct?"[^metaethics_first] "I guess there is a spectrum of concern over philosophical problems involved in building an FAI/AGI, and I'm on the far end of the that spectrum. I think most people building AGI mainly want short term benefits like profits or academic fame, and do not care as much about the far reaches of time and space, in which case they'd naturally focus more on the immediate engineering issues."[^philosophy_spectrum]|
|How much alignment work is possible early on|"My model of FAI development says that you have to get most of the way to being able to build an AGI just to be able to *start* working on many Friendliness-specific problems, and solving those problems would take a long time relative to finishing rest of the AGI capability work."[^fai_dev]|

# See also

- [List of discussions between Paul Christiano and Wei Dai]()

# External links

- [Timeline of Wei Dai publications](https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications)

[^fai_dev]: [“Wei\_Dai comments on How does MIRI Know it Has a Medium Probability of Success?”](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9j6s) LessWrong. Retrieved March 8, 2018.

[^metaethics_first]: [“Wei\_Dai comments on AALWA: Ask any LessWronger anything”](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/ap84). LessWrong. Retrieved March 8, 2018.

[^philosophy_spectrum]: [“Wei\_Dai comments on AALWA: Ask any LessWronger anything”](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/aq5j). LessWrong. Retrieved March 8, 2018.
