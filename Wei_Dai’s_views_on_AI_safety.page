---
title: Wei Dai’s views on AI safety
format: markdown
categories: AI_safety
...

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines|Wei talks a lot about concerns of shortening AI timelines (e.g. by continuing his decision theory work), but I haven't really seen him give an estimate of when he expects human-level AI to arrive.|
|Value of decision theory work|see [here](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ycj). "For example I've mostly stopped working on decision theory because it seems to help UFAI as much as FAI." from [this comment](http://lesswrong.com/lw/ah5/singularity_summit_2011_workshop_report/5ya3)|
|Value of highly reliable agent design work||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed|I think somewhere he talked about how working on intelligence amplification to solve philosophical problems is more promising. See e.g. [here](http://lesswrong.com/lw/e2k/cynical_explanations_of_fai_critics_including/7786), [this post](http://lesswrong.com/lw/6mi/some_thoughts_on_singularity_strategies/ "Wei Dai (July 13, 2011). “Some Thoughts on Singularity Strategies”. LessWrong.").|
|How "prosaic" AI will be||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Difficulty of philosophy|Philosophy is hard. Wei has discussed this in many places. See [here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o19) for one discussion. See [here](http://lesswrong.com/lw/jgz/aalwa_ask_any_lesswronger_anything/dx46) for a recent comment. I'm not aware of a single comprehensive overview of his views on the difficulty of philosophy.|
|How well we need to understand philosophy before building AGI|We need to understand philosophy well. See some of the discussions with Paul Christiano. See also threads like [this one](http://lesswrong.com/lw/ua/the_level_above_mine/8b8u).|
|How much alignment work is possible early on|"My model of FAI development says that you have to get most of the way to being able to build an AGI just to be able to *start* working on many Friendliness-specific problems, and solving those problems would take a long time relative to finishing rest of the AGI capability work."[^fai_dev]|

# See also

- [List of discussions between Paul Christiano and Wei Dai]()

# External links

- [Timeline of Wei Dai publications](https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications)

[^fai_dev]: [“Wei\_Dai comments on How does MIRI Know it Has a Medium Probability of Success?”](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9j6s) LessWrong. Retrieved March 8, 2018.