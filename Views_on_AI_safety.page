---
title: Views on AI safety
format: markdown
categories: AI_safety
...

# Views

The people who seem most knowledgeable about AI safety still seem to disagree about many things in AI safety (and also many things outside of AI safety, but that is less relevant here). It seems worth collecting some of these views into a single place.

- [Carl Shulman's views on AI safety]()
- [Dario Amodei's views on AI safety]()
- [Eliezer Yudkowsky's views on AI safety]()
- [Paul Christiano's views on AI safety]()
- [Robin Hanson's views on AI safety]()
- [Wei Dai's views on AI safety]()
- Daniel Dewey's views on AI safety
- Holden Karnofsky's views on AI safety
- Katja Grace's views on AI safety
- Owen Cotton-Barratt's views on AI safety
- Andrew Critch's views on AI safety
- Jacob Steinhardt's views on AI safety
- Stuart Armstrong's views on AI safety
- Nick Bostrom's views on AI safety

Here's the master list of topics I want to cover (each of the pages above might have a smaller list because I made them before I expanded this master list):

|Topic|Details|
|-----------------|------------------------------------------------------------|
|AI timelines|Also implications of short or long timelines (which should probably covered in the "value of" rows instead).|
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Preference ordering between kinds of AGI|e.g. some people prefer WBE because human values are more likely to be preserved, while neuromorphic AI seems difficult to understand so more difficult to control, and so forth|
|Type of AI safety work most endorsed||
|Value of decision theory work||
|Value of highly reliable agent design work||
|Value of machine learning safety work||
|Value of intelligence amplification work||
|Value of thinking of esoteric failure modes|see e.g. [this remark](https://arbital.com/p/probable_environment_hacking/#subpage-1gy)|
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|How "prosaic" AI will be||
|Difficulty of philosophy||
|How well we need to understand philosophy before building AGI||
|Cooperation vs values spreading/[moral advocacy]()||

# Consensus

It might also be good to make a list of topics on which most people basically agree (orthogonality thesis? convergent instrumental goals? human-level AGI possible?).

# Discussions

In addition, there are some discussion threads (on LessWrong and some other places) where some of these people go into long debates. Collecting some of these also seems like a good idea:

- [List of discussions between Paul Christiano and Wei Dai]()
- List of discussions between Eliezer Yudkowsky and Wei Dai
- List of discussions between Vladimir Slepnev and Wei Dai
- List of discussions between Eliezer Yudkowsky and Robin Hanson (the AI FOOM debate might be the bulk of this?)
