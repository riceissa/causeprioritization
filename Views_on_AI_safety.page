---
title: Views on AI safety
format: markdown
categories: AI_safety
...

The people who seem most knowledgeable about AI safety still seem to disagree about many things *within* AI safety. It seems worth collecting some of these views into a single place.

- [Carl Shulman's views on AI safety]()
- [Dario Amodei's views on AI safety]()
- [Eliezer Yudkowsky's views on AI safety]()
- [Paul Christiano's views on AI safety]()
- [Robin Hanson's views on AI safety]()
- [Wei Dai's views on AI safety]()
- Daniel Dewey's views on AI safety
- Holden Karnofsky's views on AI safety
- Katja Grace's views on AI safety
- Owen Cotton-Barratt's views on AI safety
- Andrew Critch's views on AI safety

Here's the master list of topics I want to cover:

|Topic|Details|
|-----------------|------------------------------------------------------------|
|AI timelines||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Preference ordering between kinds of AGI|e.g. some people prefer WBE because human values are more likely to be preserved, while neuromorphic AI seems difficult to understand so more difficult to control, and so forth|
|Type of AI safety work most endorsed||
|Value of decision theory work||
|Value of highly reliable agent design work||
|Value of machine learning safety work||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|How "prosaic" AI will be||
|Difficulty of philosophy||
|How well we need to understand philosophy before building AGI||

It might also be good to make a list of topics on which most people basically agree (orthogonality thesis? convergent instrumental goals? human-level AGI possible?).

In addition, there are some discussion threads (on LessWrong and some other places) where some of these people go into long debates. Collecting some of these also seems like a good idea:

- [List of discussions between Paul Christiano and Wei Dai]()
