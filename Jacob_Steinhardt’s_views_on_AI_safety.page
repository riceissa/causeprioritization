---
title: Jacob Steinhardtâ€™s views on AI safety
format: markdown
categories: AI_safety
...

<https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/>

<http://lesswrong.com/r/discussion/lw/iyo/the_inefficiency_of_theoretical_discovery/a07j>

<http://lesswrong.com/lw/cyy/seeking_information_relevant_to_deciding_whether/6t83>

<http://lesswrong.com/lw/6ct/siais_shortterm_research_program/4eo6>

<http://lesswrong.com/lw/673/model_uncertainty_pascalian_reasoning_and/4cvy>

from 2011: "there is some absurdly high amount of unexplored ground in areas relating to whole brain emulations and brain-computer interfaces. Neither of these are particularly math-heavy, and it's likely that at there are parts of the problem that just require engineering or lab experience. My current estimate is that the more people working on WBE, the better. Also: the larger the fraction of the WBE community that cares about existential risks, the better." ([source](http://lesswrong.com/lw/8iz/studying_psychology_which_path_should_i_take_to/5b78))

he also appears in at least one of the MIRI--Holden conversations