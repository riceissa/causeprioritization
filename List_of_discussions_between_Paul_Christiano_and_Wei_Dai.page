---
title: List of discussions between Paul Christiano and Wei Dai
format: markdown
categories: AI_safety Philosophy
...

This is a **list of discussions between Paul Christiano and Wei Dai**, on topics mainly centered around AI alignment, philosophy, and the far future.

|Start date|End date||Venue|Thread title|Topics covered|Summary
|------|------|-------|----------------------|-----------------------------------------------------|------------------------------------|
|2011-03-01|2011-03-02|LessWrong|["Some Considerations Against Short-Term and/or Explicit Focus on Existential Risk Reduction"](http://lesswrong.com/lw/4li/some_considerations_against_shortterm_andor/3m7o)|||
|2011-04-02||LessWrong|["Anthropics in a Tegmark Multiverse"](http://lesswrong.com/lw/535/anthropics_in_a_tegmark_multiverse/)|Anthropics, UDT||
|2011-04-02|2011-04-03|LessWrong|["Where does uncertainty come from?"](http://lesswrong.com/lw/534/where_does_uncertainty_come_from/3ted)|||
|2011-04-06|2011-04-07|LessWrong|["What Should I Do?"](http://lesswrong.com/lw/546/what_should_i_do/3udd)|Boredom, research vs earning to give||
|2011-12-29||LessWrong|["Negentropy Overrated?"](http://lesswrong.com/lw/92s/negentropy_overrated/5jpv)|||
|2012-04-26||LessWrong|["Formalizing Value Extrapolation"](http://lesswrong.com/lw/c0k/formalizing_value_extrapolation/)|||
|2013-01-27|2013-02-09|Rational Altruist|["Taxonomy of change"](https://rationalaltruist.com/2013/01/27/breakdown-of-progress/)|||
|2013-02-27|2013-02-28|LessWrong|["Why might the future be good?"](http://lesswrong.com/lw/gtr/why_might_the_future_be_good/8j6j)|||
|2013-05-08|2016-03-19|LessWrong|["Pascal's Muggle: Infinitesimal Priors and Strong Evidence"](http://lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/8xrk)|AI alignment, act-based agents, expected damage of philosophical errors|Wei argues for progress in philosophy and for AI designs that can correct their philosophical errors, and thinks philosophical errors are already causing great damage in expectation. Paul is skeptical that philosophical errors are causing great damage.|
|2013-06-06||LessWrong|["Tiling Agents for Self-Modifying AI (OPFAI #2)"](http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/943m)|||
|2013-06-14||LessWrong|["After critical event W happens, they still won't believe you"](http://lesswrong.com/lw/hp5/after_critical_event_w_happens_they_still_wont/95r8)|||
|2013-07-18||LessWrong|[“Three Approaches to ‘Friendliness’ ”](http://lesswrong.com/lw/hzs/three_approaches_to_friendliness/9egp)|||
|2013-08-28|2013-08-31|LessWrong|["Outside View(s) and MIRI's FAI Endgame"](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9npb)|Difficulty of an intelligence explosion, philosophical problems, value drift||
|2014-03-01||LessWrong|["Self-Congratulatory Rationalism"](http://lesswrong.com/lw/jq7/selfcongratulatory_rationalism/amw1)|Rationality, agreement between rational agents||
|2014-12-12||LessWrong|["Approval-directed agents"](http://lesswrong.com/r/discussion/lw/ldw/approvaldirected_agents/br3w)|||
|2015-04-10||LessWrong|[“Three Approaches to ‘Friendliness’ ”](http://lesswrong.com/lw/hzs/three_approaches_to_friendliness/c8oz)|||
|2015-04-16||Medium|["Handling errors with arguments"](https://ai-alignment.com/handling-errors-with-arguments-1f34a04ccbff)|||
|2016-02-23||Medium|["ALBA: An explicit proposal for aligned AI"](https://ai-alignment.com/alba-an-explicit-proposal-for-aligned-ai-17a55f60bbcf)|||
|2016-03-10|2016-03-19|LessWrong|["AlphaGo versus Lee Sedol"](http://lesswrong.com/r/discussion/lw/ne1/alphago_versus_lee_sedol/d5sy)|Paul Christiano's approach to AI alignment, feasibility of getting an AI to defer to humans for philosophical judgment||
|2016-10-14||Facebook|["I used to think of AI control as mostly unrelated to AI security. Now I’m not even certain that they should be separate research areas."](https://www.facebook.com/paulfchristiano/posts/10210804207639074)|||
|2016-10-22||Intelligent Agent Foundations Forum|["Control and security"](https://agentfoundations.org/item?id=1045)|||
|2016-10-26||Medium|["Security amplification"](https://ai-alignment.com/security-amplification-f4931419f903)|||
|2016-11-14||Medium|["Handling destructive technology"](https://ai-alignment.com/handling-destructive-technology-85800a12d99)|||
|2016-11-22|2016-11-24|LessWrong|["Less costly signaling"](http://lesswrong.com/lw/o5h/less_costly_signaling/dhu2?context=1)|Signaling, altruism, selfishness||
|2016-12-01|2017-09-17|LessWrong|["Optimizing the news feed"](http://lesswrong.com/r/discussion/lw/o7e/optimizing_the_news_feed/dino)|Facebook newsfeed's alignment with user preferences, profits vs social welfare, tech companies' concern about public image|Paul is optimistic about tech companies like Facebook working toward greater alignment with user preferences (as opposed to ad revenue), while Wei is pessimistic.|
|2016-12-03|2016-12-05|LessWrong|["Crowdsourcing moderation without sacrificing quality"](http://lesswrong.com/lw/o7p/crowdsourcing_moderation_without_sacrificing/diqv)|Potential automated attacks on discussion forums||
|2016-12-30||Intelligent Agent Foundations Forum|["My current take on the Paul-MIRI disagreement on alignability of messy AI"](https://agentfoundations.org/item?id=1150)|||
|2017-03-19||Medium|["Benign model-free RL"](https://ai-alignment.com/benign-model-free-rl-4aae8c97e385)|||
|2017-06-10||Medium|["Corrigibility"](https://ai-alignment.com/corrigibility-3039e668638)|||
|2017-06-21|2017-06-26|LessWrong|["S-risks: Why they are the worst existential risks, and how to prevent them"](http://lesswrong.com/lw/p5v/srisks_why_they_are_the_worst_existential_risks/duaz)|Suffering risks, moral uncertainty, suffering-hating civilizations, whether it's good for certain civilizations to exist (under different value assumptions)|Wei pushes for progress in philosophy (decision theory, meta-ethics, and so on). Paul assumes a purely suffering-focused view to understand its recommendations.|
|2017-07-09|2017-07-15|Effective Altruism Forum|["My current thoughts on MIRI's 'highly reliable agent design' work"](http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/bcg)|Maturity and concreteness of MIRI's highly reliable agent design (HRAD) approach vs Paul Christiano's approach to AI alignment|Wei is worried that Paul's approach has not received the level of scrutiny that MIRI's approach has; Paul defends his approach.|
|2017-07-17||Intelligent Agent Foundations Forum|["Current thoughts on Paul Christano's research agenda"](https://agentfoundations.org/item?id=1589)|||
|2017-07-17||Intelligent Agent Foundations Forum|["Current thoughts on Paul Christano's research agenda"](https://agentfoundations.org/item?id=1590)|||
|2017-08-18||Intelligent Agent Foundations Forum|["Autopoietic systems and difficulty of AGI alignment"](https://agentfoundations.org/item?id=1631)|||
|2018-02-25||LesserWrong|["The abruptness of nuclear weapons"](https://www.lesserwrong.com/posts/y5eapqjYYku8Wt9wn/the-abruptness-of-nuclear-weapons#Y7hSrxDyHTCixLLYu)|||
|2018-03-08||LesserWrong|["Prize for probable problems"](https://www.lesserwrong.com/posts/SqcPWvvJJwwgZb6aH/prize-for-probable-problems)|Paul Christiano's approach to alignment||
|2018-03-10||Medium|["Universality and security amplification"](https://ai-alignment.com/universality-and-security-amplification-551b314a3bab)|||

(I'm actually not sure I got all the Medium threads because I find [Medium profiles](https://medium.com/@weidai) confusing to browse.)

# See also

- [Dario Amodei's views on AI safety]()

# External links

- [Timeline of Wei Dai publications](https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications)
- [AI FOOM debate](https://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate)
