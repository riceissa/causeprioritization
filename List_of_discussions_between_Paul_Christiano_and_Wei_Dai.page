---
title: List of discussions between Paul Christiano and Wei Dai
format: markdown
categories: AI_safety Philosophy
...

This is a **list of discussions between Paul Christiano and Wei Dai**, on topics mainly centered around AI alignment and philosophy.

|Date range|Venue|Thread title|Topics covered|Summary
|------|-------|----------------------|-----------------------------------------------------|------------------------------------|
|2012-04-26--|LessWrong|["Formalizing Value Extrapolation"](http://lesswrong.com/lw/c0k/formalizing_value_extrapolation/)|||
|2016-03-10--2016-03-19|LessWrong|["AlphaGo versus Lee Sedol"](http://lesswrong.com/r/discussion/lw/ne1/alphago_versus_lee_sedol/d5sy)|Paul Christiano's approach to AI alignment, feasibility of getting an AI to defer to humans for philosophical judgment||
|2016-11-22--2016-11-24|LessWrong|["Less costly signaling"](http://lesswrong.com/lw/o5h/less_costly_signaling/dhu2?context=1)|Signaling, altruism, selfishness||
|2016-12-01--2017-09-17|LessWrong|["Optimizing the news feed"](http://lesswrong.com/r/discussion/lw/o7e/optimizing_the_news_feed/dino)|Facebook newsfeed's alignment with user preferences, profits vs social welfare, tech companies' concern about public image|Paul is optimistic about tech companies like Facebook working toward greater alignment with user preferences (as opposed to ad revenue), while Wei is pessimistic.|
|2016-12-03--2016-12-05|LessWrong|["Crowdsourcing moderation without sacrificing quality"](http://lesswrong.com/lw/o7p/crowdsourcing_moderation_without_sacrificing/diqv)|Potential automated attacks on discussion forums||
|2017-06-21--2017-06-26|LessWrong|["S-risks: Why they are the worst existential risks, and how to prevent them"](http://lesswrong.com/lw/p5v/srisks_why_they_are_the_worst_existential_risks/duaz)|Suffering risks, moral uncertainty, suffering-hating civilizations, whether it's good for certain civilizations to exist (under different value assumptions)|Wei pushes for progress in philosophy (decision theory, meta-ethics, and so on). Paul assumes a purely suffering-focused view to understand its recommendations.|
|2017-07-09--2017-07-15|Effective Altruism Forum|["My current thoughts on MIRI's 'highly reliable agent design' work"](http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/bcg)|Maturity and concreteness of MIRI's highly reliable agent design (HRAD) approach vs Paul Christiano's approach to AI alignment|Wei is worried that Paul's approach has not received the level of scrutiny that MIRI's approach has; Paul defends his approach.|

<https://www.greaterwrong.com/posts/y5eapqjYYku8Wt9wn/the-abruptness-of-nuclear-weapons#comment-XE5bPgcFwFZ53K9sR>

<https://ai-alignment.com/benign-model-free-rl-4aae8c97e385> (scroll down to comments)

maybe more at <https://medium.com/@weidai>

more at <https://agentfoundations.org/threads?id=Wei_Dai> and on LW

# See also

- [Dario Amodei's views on AI safety]()

# External links

- [Timeline of Wei Dai publications](https://timelines.issarice.com/wiki/Timeline_of_Wei_Dai_publications)
- [AI FOOM debate](https://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate)
