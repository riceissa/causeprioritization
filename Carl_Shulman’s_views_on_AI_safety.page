---
title: Carl Shulman’s views on AI safety
format: markdown
categories: AI_safety
...

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines||
|Value of highly reliable agent design (e.g. decision theory, logical uncertainty) work||
|Value of intelligence amplification work|See comments like [this one](http://lesswrong.com/lw/384/genetically_engineered_intelligence/32jj)|
|Value of pushing for whole brain emulation|He gives some points against in a comment starting with "However, the conclusion that accelerating WBE (presumably via scanning or neuroscience, not speeding up Moore's Law type trends in hardware) is the best marginal project for existential risk reduction is much less clear."[^wbe_unclear]|
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed||
|How "prosaic" AI will be||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||

[^wbe_unclear]: [“CarlShulman comments on Hedging our Bets: The Case for Pursuing Whole Brain Emulation to Safeguard Humanity's Future”](http://lesswrong.com/lw/1s3/hedging_our_bets_the_case_for_pursuing_whole/1oyg). LessWrong. Retrieved March 8, 2018.
