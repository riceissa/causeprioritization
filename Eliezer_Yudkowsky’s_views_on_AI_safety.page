---
title: Eliezer Yudkowsky’s views on AI safety
format: markdown
categories: AI_safety
...

<http://lesswrong.com/lw/gfb/update_on_kim_suozzi_cancer_patient_in_want_of/8bv0>

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines|see [here](https://www.lesswrong.com/posts/xgr8sDtQEEs7zfTLH/update-on-kim-suozzi-cancer-patient-in-want-of-cryonics#uE6zdxS4ez9cvNo4o) for 2083 as Carl's estimate, and in [IEM](https://intelligence.org/files/IEM.pdf) Eliezer says "I’m currently trying to sort out with Carl Shulman why my median is forty-five years in advance of his median" (p. 83) so that puts Eliezer's estimate as of 2013 at around 2038. See also [this thread](https://www.greaterwrong.com/posts/CZQuFoqgPXQawH9aL/new-report-intelligence-explosion-microeconomics/comment/9WX7cPHmAuoseWS4G) and [this tweet](https://twitter.com/ESYudkowsky/status/1164332124712738821) and [this comment](https://www.econlib.org/archives/2016/03/so_far_my_respo.html/#comment-158703).|
|Value of decision theory work||
|Value of highly reliable agent design work||
|Difficulty of AI alignment|See [this tweet](https://twitter.com/ESYudkowsky/status/1164332124712738821).|
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed||
|How "prosaic" AI will be||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Difficulty of philosophy|[here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o19) is one remark|
