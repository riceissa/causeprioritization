---
title: Eliezer Yudkowskyâ€™s views on AI safety
format: markdown
categories: AI_safety
...

<http://lesswrong.com/lw/gfb/update_on_kim_suozzi_cancer_patient_in_want_of/8bv0>

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines||
|Value of decision theory work||
|Value of highly reliable agent design work||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed||
|How "prosaic" AI will be||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Difficulty of philosophy|[here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9o19) is one remark|
