---
title: Splitting up tasks
format: markdown
categories: AI_safety
...

I'm not sure what the general name for this is. There seems to be differing intuitions about how much you can take apart tasks and accomplish the whole task using subprocesses that are limited.

But see e.g. [this thread](https://medium.com/@weidai/is-h%E1%B4%AC-capable-of-learning-a-new-skill-that-h-and-a-dont-have-db95aeb7d8e), [this post](https://www.facebook.com/bshlgrs/posts/10212280147893588).

Quote from [IEM](https://intelligence.org/files/IEM.pdf):

> It is a general truth of computer science that if you take one processing unit and split it up into ten parts with limited intercommunication bandwidth, they can do no better than the original on any problem, and will do considerably worse on many problems.

Possibly related to ideas like [Divide and conquer algorithm](https://en.wikipedia.org/wiki/Divide_and_conquer_algorithm), [Diseconomies of scale](https://en.wikipedia.org/wiki/Diseconomies_of_scale), and the Polymath Project.
