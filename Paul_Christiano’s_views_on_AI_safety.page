---
title: Paul Christianoâ€™s views on AI safety
format: markdown
categories: AI_safety
...

|Topic|View|
|-----------------|------------------------------------------------------------|
|AI timelines||
|Value of decision theory work||
|Value of highly reliable agent design work||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|Type of AI safety work most endorsed||
|How "prosaic" AI will be|Prosaic AGI is possible. See [here](https://ai-alignment.com/prosaic-ai-control-b959644d79c2), where he gives \>10% probability to building prosaic AGI.|
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|How well we need to understand philosophy before building AGI|It's unlikely or at least unclear that we need to solve many philosophical problems. See [here](http://lesswrong.com/lw/ig9/outside_views_and_miris_fai_endgame/9npb) for one expression of the idea.|

# See also

- [List of discussions between Paul Christiano and Wei Dai]()

# External links

- ["My current take on the Paul-MIRI disagreement on alignability of messy AI"](https://agentfoundations.org/item?id=1129) by Jessica Taylor
- ["Current thoughts on Paul Christano's research agenda"](https://agentfoundations.org/item?id=1534) by Jessica Taylor
