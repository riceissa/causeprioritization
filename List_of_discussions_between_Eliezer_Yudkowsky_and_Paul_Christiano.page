---
title: List of discussions between Eliezer Yudkowsky and Paul Christiano
format: markdown
categories: AI_safety
...

This is a **list of discussions between Eliezer Yudkowsky and Paul Christiano**.

|Start date|End date|Venue|Thread title|Topics covered|Summary
|------|------|-------|----------------------|-----------------------------------------------------|------------------------------------|
|2010-12-18||LessWrong|["Cryptographic Boxes for Unfriendly AI"](https://www.lesswrong.com/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai#oY8omk4m4rZcsA2MG)|||
|2010-12-22||LessWrong|["Motivating Optimization Processes"](https://www.lesswrong.com/posts/oatMFQjAzEnuzti7u/motivating-optimization-processes)|||
|2012-02-26||LessWrong|["The mathematics of reduced impact: help needed"](https://www.lesswrong.com/posts/8Nwg7kqAfCM46tuHq/the-mathematics-of-reduced-impact-help-needed#WMuBtqiNoAhid2P9c)|||
|2013-06-12||LessWrong|["Do Earths with slower economic growth have a better chance at FAI?"](https://www.lesswrong.com/posts/FS6NCWzzP8DHp4aD4/do-earths-with-slower-economic-growth-have-a-better-chance#Q4DRwQ2rSyTja26fR)|||
|2013-06-13||LessWrong|["After critical event W happens, they still won't believe you"](https://www.lesswrong.com/posts/LNKh22Crr5ujT85YM/after-critical-event-w-happens-they-still-won-t-believe-you#WrJkgbAMunHRx5Rrg)|||
|2015-06-17|2015-12-29|Arbital|["Mindcrime"](https://arbital.com/p/mindcrime/#subpage-78)|||
|2015-06-18|2015-06-18|Arbital|["Diamond maximizer"](https://arbital.com/p/diamond_maximizer/#subpage-7t)|||
|2015-06-18|2015-06-18|Arbital|["Identifying ambiguous inductions"](https://arbital.com/p/inductive_ambiguity/)|||
|2015-06-18|2015-06-18|Arbital|["Patch resistance"](https://arbital.com/p/patch_resistant/)|||
|2015-06-18|2015-06-18|Arbital|["Relevant limited AI"](https://arbital.com/p/relevant_limited_AI/)|||
|2015-06-18|2015-06-18|Arbital|["Zermelo-Fraenkel provability oracle"](https://arbital.com/p/ZF_provability_oracle/)|||
|2015-06-18|2015-07-14|Arbital|["Complexity of value"](https://arbital.com/p/complexity_of_value/#subpage-7h)|||
|2015-06-18|2015-07-14|Arbital|["Omnipotence test for AI safety"](https://arbital.com/p/omni_test/)|||
|2015-06-18|2015-12-27|Arbital|["Ontology identification problem"](https://arbital.com/p/ontology_identification/)|||
|2015-06-18|2015-12-27|Arbital|["Ontology identification problem"](https://arbital.com/p/ontology_identification/)|||
|2015-06-18|2016-03-22|Arbital|["Nearest unblocked strategy"](https://arbital.com/p/nearest_unblocked/)|||
|2015-06-19|2015-12-29|Arbital|["Distant superintelligences can coerce the most probable environment of your AI"](https://arbital.com/p/probable_environment_hacking/)|||
|2015-11-10|2017-11-11|Facebook|["Want to avoid going down an awful lot of blind alleys in AI safety? Here's a general heuristic ..."](https://www.facebook.com/yudkowsky/posts/10153748345169228)|||
|2015-12-03|2015-12-06|Medium|["On heterogeneous objectives"](https://ai-alignment.com/on-heterogeneous-objectives-b38d0e003399)|||
|2015-12-27|2015-12-27|Arbital|["Behaviorist genie"](https://arbital.com/p/behaviorist/)|||
|2015-12-27|2015-12-29|Arbital|["Orthogonality Thesis"](https://arbital.com/p/orthogonality/#subpage-1fr)|||
|2015-12-27|2016-04-21|Arbital|["AI safety mindset"](https://arbital.com/p/AI_safety_mindset/)|||
|2015-12-29|2015-12-29|Arbital|["Autonomous AGI"](https://arbital.com/p/Sovereign/)|||
|2015-12-29|2015-12-29|Arbital|["Modeling distant superintelligences"](https://arbital.com/p/distant_SIs/)|||
|2015-12-29|2016-01-03|Arbital|["Known-algorithm non-self-improving agent"](https://arbital.com/p/KANSI/)|||
|2015-12-29||Arbital|["Task-directed AGI"](https://arbital.com/p/task_agi/)|||
|2016-01-01|2016-01-01|Arbital|["Advanced agent properties"](https://arbital.com/p/advanced_agent/#subpage-1j8)|||
|2016-01-30|2016-01-30|Arbital|["Natural language understanding of 'right' will yield normativity"](https://arbital.com/p/4s/)|||
|2016-02-25|2016-02-29|Arbital|["Epistemic and instrumental efficiency"](https://arbital.com/p/efficiency/#subpage-23w)|||
|2016-03-09||Arbital|["Reflectively consistent degree of freedom"](https://arbital.com/p/reflective_degree_of_freedom/)|||
|2016-03-11|2016-03-13|Facebook|["(Long.) As I post this, AlphaGo seems almost sure to win the third game and the match ..."](https://www.facebook.com/yudkowsky/posts/10154018209759228)|||
|2016-03-16||Arbital|["Open subproblems in aligning a Task-based AGI"](https://arbital.com/p/taskagi_open_problems/)|||
|2016-03-19|2016-03-19|Arbital|["Low impact"](https://arbital.com/p/low_impact/)|||
|2016-03-26|2016-03-26|Arbital|["Informed oversight"](https://arbital.com/p/informed_oversight/)|||
|2016-03-29|2016-03-29|Facebook|["Paul Christiano, someone wrote a story about approval-directed agents! ..."](https://www.facebook.com/yudkowsky/posts/10154092882824228)|||
|2016-04-15|2016-04-17|Arbital|["Faithful simulation"](https://arbital.com/p/faithful_simulation/)|||
|2016-04-15|2016-04-21|Arbital|["Goal-concept identification"](https://arbital.com/p/identify_goal_concept/)|||
|2016-04-29|2016-06-06|Arbital|["Coherent extrapolated volition (alignment target)"](https://arbital.com/p/cev/)|||
|2016-05-17|2016-05-18|Arbital|["Show me what you've broken"](https://arbital.com/p/show_broken/)|||
|2016-10-21|2016-10-21|Facebook|["What people discuss at AI ethics conferences: How we can possibly convey all the deep subtleties of human morality ..."](https://www.facebook.com/yudkowsky/posts/10154690145854228?comment_id=10154690164674228&comment_tracking=%7B%22tn%22%3A%22R9%22%7D)|||
|2017-01-17|2017-01-17|Facebook|["I am concerned about the number of people I've heard joking about Trump's election being evidence for the Simulation Hypothesis ..."](https://www.facebook.com/yudkowsky/posts/10154981140854228?comment_id=10154981431689228&comment_tracking=%7B%22tn%22%3A%22R9%22%7D)|||
|2017-10-19|2017-10-19|Facebook|["AlphaGo Zero uses 4 TPUs, is built entirely out of neural nets with no handcrafted features ..."](https://www.facebook.com/yudkowsky/posts/10155848910529228)|||
|2017-12-09|2017-12-11|Facebook|["Max Tegmark put it well, on Twitter: The big deal about Alpha Zero isn't ..."](https://www.facebook.com/yudkowsky/posts/10155992246384228?comment_id=10155996489019228&comment_tracking=%7B%22tn%22%3A%22R1%22%7D)|||

# See also

- [List of discussions between Paul Christiano and Wei Dai]()

# External links

- ["My current take on the Paul-MIRI disagreement on alignability of messy AI"](https://agentfoundations.org/item?id=1129) by Jessica Taylor
