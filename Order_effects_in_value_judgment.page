---
title: Order effects in value judgment
format: markdown
categories: Philosophy
...

<https://foundational-research.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf> has a section that talks about order effects and links to <http://www.philosophyexperiments.com/sedan/Default5.aspx>.

see also ["Order Effects in Moral Judgment"](https://pdfs.semanticscholar.org/23b4/9e2f491f9e460828797ba115dac89d1a1753.pdf).

wei dai also talks about this in his LW comments (or maybe a post?)

One of the "meta-ethical alternatives" Wei Dai gives is:[^metaethical]

> None of the above facts exist, and reflecting on what one wants turns out to be a divergent process (e.g., it's highly sensitive to initial conditions, like whether or not you drank a cup of coffee before you started, or to the order in which you happen to encounter philosophical arguments). There are still facts about rationality, so at least agents that are already rational can call their utility functions (or the equivalent of utility functions in whatever decision theory ends up being the right one) their real values.

And:[^ref_eq]

> I often wonder and ask others what non-trivial properties we can state about moral reasoning (i.e., besides that theoretically it must be some sort of an algorithm). One thing that I don't think we know yet is that for any given human, their moral judgments/intuitions are guaranteed to converge to some stable and coherent set as time goes to infinity. It may well be the case that there are multiple eventual equilibria that depend on the order in which one considers arguments, or none if for example their conclusions keep wandering chaotically among several basins of attraction as they review previously considered arguments. So I think the singular term "reflective equilibrium" is currently unjustified when talking about someone's eventual conclusions, and we should instead use "the possibly null set of eventual reflective equilibria". (Unless someone can come up with a pithier term that has similar connotations and denotations.)

And:[^future]

> I’m envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won’t be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.

[^metaethical]: Wei Dai. [“Six Plausible Meta-Ethical Alternatives”](http://lesswrong.com/lw/khf/six_plausible_metaethical_alternatives/). LessWrong. Retrieved February 21, 2018.

[^ref_eq]: [“Wei_Dai comments on Open Thread, January 1-15, 2013 - Less Wrong”](http://lesswrong.com/lw/g66/open_thread_january_115_2013/86tq). LessWrong. Retrieved February 21, 2018.

[^future]: [“I hope you stay engaged with the AI risk discussions and maintain your credibili...”](https://agentfoundations.org/item?id=1636). Intelligent Agent Foundations Forum. Retrieved February 21, 2018.
