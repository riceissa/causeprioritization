---
title: Existential risks
format: markdown
categories: Cause_areas, prize
...

<!-- keywords:
exrisk xrisk x-risk
-->

An existential risk is one that threatens the entire future of humanity. More specifically, existential risks are those that threaten the extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development.

# Types

Nick Bostrom classifies existential risks in the following categories^[[Existential Risks](https://nickbostrom.com/existential/risks.html#_ftnref7)]:

> **Bangs** – Earth-originating intelligent life goes extinct in relatively sudden disaster resulting from either an accident or a deliberate act of destruction.  
> **Crunches** – The potential of humankind to develop into posthumanity[7] is permanently thwarted although human life continues in some form.  
> **Shrieks** – Some form of posthumanity is attained but it is an extremely narrow band of what is possible and desirable.  
> **Whimpers** – A posthuman civilization arises but evolves in a direction that leads gradually but irrevocably to either the complete disappearance of the things we value or to a state where those things are realized to only a minuscule degree of what could have been achieved.

Anthropogenic risks are those that would be the result of human activity, and natural risks are those that would be caused by nature. There could also be risks from alien civilizations.

# Interventions
**[Forecasting]()**

**[Movement building]()**

**Retain a last-resort readiness for preemptive action**

**[Global coordination]()**

**[Recovery from a global catastrophe]()**

**Differential technological development** (see: [Differential progress]())

- [Intelligence enhancement]()
- [Surveillance]()
- [Embryo selection]()

**Support programs that directly reduce specific existential risks**

# List of existential risks

## By causes

Anthropogenic risks can be further categorized by whether they are intentional or accidental, and looking at the associated economic incentives.

Technological x-risks:

- [AI safety]() (related: [Whole brain emulation]())
- [Biosecurity]()
- [Climate change]()
- [Nuclear security]()
- [Physical disasters]()
- Other: [Emerging technologies assessment]()

Other anthropogenic x-risks:

- [Population control]()
- [Preventing totalitarianism]()

Natural x-risks:

- [Asteroids]()
- [Geomagnetic storms]()

Other:

- [Aliens]()
- Simulation shutdown (see: [Simulation hypothesis]())

## By mechanism
Bangs:

- [Nanotechnology]()
- Nuclear holocaust (see: [Nuclear security]())
- Simulation shutdown (see: [Simulation hypothesis]())
- Badly programmed superintelligence (see: [AI safety]())
- Bioengineered pandemics (see: [Biosecurity]())
- [Physical disasters]()
- Naturally occuring pandemics (see: [Biosecurity]())
- [Asteroid or comet impact]()
- Runaway global warming (see [Climate change](https://causeprioritization.org/Climate_change))

Crunches:

- Resource depletion or ecological destruction (see [Environmentalism]())
- Misguided world government or another static social equilibrium stops technological progress (see [Preventing totalitarianism]())
- “Dysgenic” pressures
- [Technological arrest]()

Shrieks:

- Take-over by a transcending upload (see [Whole brain emulation]())
- Flawed superintelligence (see [AI safety]())
- Repressive totalitarian global regime (see [Preventing totalitarianism]())

Whimpers:

- Our potential or even our core values are eroded by evolutionary development (see [Population control]())
- Killed by an extraterrestrial civilization (see [Aliens]())

## By type of risk^[[How to Understand and Mitigate Risk](https://www.lesswrong.com/posts/eA9a5fpi6vAmyyp74/how-to-understand-and-mitigate-risk)]

Transparent risks

Opaque risks

Knightian risks:

- Black swans
- Dynamic environment
- Adversarial environments

Mitigation of Knightian risks:

- Antifragility
- Effectuation
- Capability enhancement

# Prize
The Future of Life Institute offers the Future of Life Award to individuals that help prevent existential risks ([source](https://futureoflife.org/2017/10/27/55-years-preventing-nuclear-attack-arkhipov-honored-inaugural-future-life-award/)).

# Organizations
Organizations working on a specific existential risk are only mentioned on the specific page for existential risk.

Main focus:  

- [Survival and Flourishing](http://survivalandflourishing.org/)
- [Future of Humanity Institute](https://en.wikipedia.org/wiki/Future_of_Humanity_Institute)
- [Future of Life Institute](https://en.wikipedia.org/wiki/Future_of_Life_Institute)  
- [Global Catastrophic Risk Institute](http://gcrinstitute.org/)  
- [The Cambridge Centre for the Study of Existential Risk](https://en.wikipedia.org/wiki/Centre_for_the_Study_of_Existential_Risk)  
- [The Institute for Ethics and Emerging Technology](https://en.wikipedia.org/wiki/Institute_for_Ethics_and_Emerging_Technologies)  
- [Machine Intelligence Research Institute](https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute)  
- [Center for Security and Emerging Technology](https://cset.georgetown.edu/about-us/)  
- [OpenAI](https://en.wikipedia.org/wiki/OpenAI)  
- [AllFed](http://allfed.info/)

Related:  

- [Center for New American Security](https://www.cnas.org)

# Grants

The Open Philanthropy Project with Good Venture is one of the main founders in this space as can be seen in their [grant database](https://www.openphilanthropy.org/giving/grants).

Other major grantors include [Survival and Flourishing](http://survivalandflourishing.org/#past-grant-recommendations) and the [Future of Humanity Institute](https://en.wikipedia.org/wiki/Future_of_Humanity_Institute).

# See also
- [Risks of astronomical suffering]()
- [Importance]()

# External links  
- [Existential risks website](http://www.existential-risk.org)
- [Existential risks by Nick Bostrom](https://nickbostrom.com/existential/risks.html)
- [Open Philanthropy Project review of Global Catastrophic Risks](https://www.openphilanthropy.org/research/cause-reports/global-catastrophic-risks/global-catastrophic-risks)
- [Global catastrophic risk on Wikipedia](https://en.wikipedia.org/wiki/Global_catastrophic_risk)
- [The case for reducing extinction risk by 80,000 Hours](https://80000hours.org/articles/extinction-risk/)
- [Existential risk by the Future of Life Institute](https://futureoflife.org/background/existential-risk/)
- [This comment](http://effective-altruism.com/ea/9f/on_progress_and_prosperity/12p) that considers a "scorched earth" strategy in the context of differential intellectual progress (this is also something to think about regarding SIMADs).