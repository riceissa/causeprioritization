---
title: Daniel Deweyâ€™s views on AI safety
format: markdown
categories: AI_safety
...

|Topic|Views|
|-----------------|------------------------------------------------------------|
|AI timelines||
|Kind of AGI we will have first (de novo, neuromorphic, WBE, etc.)||
|Preference ordering between kinds of AGI||
|Type of AI safety work most endorsed||
|Value of decision theory work||
|Value of highly reliable agent design work|See [this post](http://effective-altruism.com/ea/1ca/my_current_thoughts_on_miris_highly_reliable/).|
|Value of machine learning safety work||
|Value of intelligence amplification work||
|Value of thinking of esoteric failure modes||
|Difficulty of AI alignment||
|Shape of takeoff/discontinuities in progress||
|How "prosaic" AI will be||
|Difficulty of philosophy||
|How well we need to understand philosophy before building AGI||
|Cooperation vs values spreading/[moral advocacy]()||

# See also

- [Views on AI safety]()
