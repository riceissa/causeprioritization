---
title: Template for views on AI safety
format: markdown
categories: AI_safety
...

# Paths to AGI

|Path|Timeline for when we reach AGI|Probability that we first reach AGI using this path|Safety rating|Implications (e.g. emergence of singleton)|
|----|------------------------------|---------------------------------------------------|-------------|------------------------------------------|
|De novo|||||
|Neuromorphic|||||
|Whole brain emulation|||||
|Intelligence enhancement|||||

# Approaches to alignment

|Approach|Time/resource cost to achieve alignment|Probability of this approach working in principle (i.e. ignoring AI timelines)|How competitive this approach would be with unalighted AGI|Number of serial discoveries needed/how parallelizable the approach is|
|--------|---------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------|
|Highly reliable agent design|
|Paul Christiano's approach (are there multiple?)|
|Inverse reinforcement learning|
|Learning from human preferences|
|Adversarial examples|
|Working on philosophical questions|
|Indirect normativity|
|Coherent extrapolated volition|

# The role of philosophy

# Miscellaneous questions

|Topic|
|-----|
|Hardware overhang|
|Openness vs secrecy|
|Race dynamics|
|Differential development|
|Ability reduce problems to learning problems; see [here](https://arbital.com/p/taskagi_open_problems/)|
