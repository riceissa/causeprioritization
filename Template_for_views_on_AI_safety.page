---
title: Template for views on AI safety
format: markdown
categories: AI_safety
...

# Paths to AGI

|Path|Timeline for when we reach AGI|Probability that we first reach AGI using this path|Safety rating|Implications (e.g. emergence of singleton, takeoff shape)|
|----|------------------------------|---------------------------------------------------|-------------|------------------------------------------|
|De novo|||||
|Neuromorphic|||||
|Whole brain emulation|||||
|Intelligence enhancement|||||

# Approaches to alignment

|Approach|Time/resource cost to achieve alignment|Probability of this approach working in principle (i.e. ignoring AI timelines)|How competitive this approach would be with unaligned AGI|Number of serial discoveries needed/how parallelizable the approach is|
|--------|---------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------|
|Highly reliable agent design|
|Paul Christiano's approach (are there multiple?)|
|Inverse reinforcement learning|
|Learning from human preferences|
|Adversarial examples|
|Working on philosophical questions|
|Indirect normativity|
|Coherent extrapolated volition|

# The role of philosophy

Eliezer has said something to the effect that "copy-pasting a strawberry hits
95% of the interesting alignment problems", but he has also said we can't do
with anything less than full human morality, or something similar. Wei Dai
[pointed this out](https://www.facebook.com/yudkowsky/posts/10154690145854228?comment_id=10154691157704228&comment_tracking=%7B%22tn%22%3A%22R%22%7D)
in a Facebook thread. I think this is related to the "how much
philosophy do we need to understand?" question but probably distinct.

# Miscellaneous questions

|Topic|
|-----------------------|
|Hardware overhang|
|Openness vs secrecy|
|Race dynamics|
|Differential development/stuff about desirability of slow technological development|
|Ability reduce problems to learning problems; see [here](https://arbital.com/p/taskagi_open_problems/)|
|Amount of hardware required for first AGI|
|State involvement|
|Ceiling for artificial intelligence (e.g. some people think AGI isn't even possible in principle, and even those who believe AGI is possible have different views on how much smarter than a human a AGI could be)|

