---
format: markdown
categories: artificial-intelligence a-cause existential-risks
...

# Summary

Importance
:    \<importance rating\>

Tractability
:    \<tractability rating\>

Neglectedness
:    \<neglectedness rating\>


<!-- keywords: ai siai miri singularity eliezer yudkowsky agi
               asi fai friendly ai
-->

[AI Impacts](http://www.aiimpacts.org/) is an informational site that "aims to improve our understanding of the likely impacts of human-level artificial intelligence".

The main EA organization working in this field seems to be [MIRI](http://intelligence.org/), which does relevant math research and sponsors forecasting projects like [AI Impacts](http://www.aiimpacts.org/).
FHI might also have more information.

[*Superintelligence: Paths, Dangers, Strategies*](http://www.amazon.com/dp/0199678111/ref=cm_sw_su_dp) by [Nick Bostrom](http://www.nickbostrom.com/) lays out a foundation for navigating scenarios where machine brains surpass human brains in general intelligence.

http://www.scholarpedia.org/article/Artificial_General_Intelligence

http://www.reddit.com/r/artificial/comments/2qyg8j/how_large_do_you_think_the_first_strong_ai/cnathbn

http://www.reddit.com/r/artificial/

http://bicasociety.org/cogarch/architectures.htm

# Importance

FIXME

# Tractability

FIXME

# Neglectedness

FIXME

# See also

- [Simulation hypothesis]()
- [CarlShulman comments on How does MIRI Know it Has a Medium Probability of Success?](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9i5b)
