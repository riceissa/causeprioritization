---
title: Dario Amodeiâ€™s views on AI safety
format: markdown
categories: AI_safety
...

from [here](http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/#comment-128569):

> Full Disclosure: I'm friends with Dario and know things through him I can't share here. I've also outsourced my opinion on AI risk to him since before he was working at OpenAI.

From [this video](https://www.youtube.com/watch?v=JA4vW4oQavk&index=24&list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN) (this is a really rough transcript right now -- i'll clean it up soon):

> So from the EA community in particular I mean I think there's a lot of things the EA community gets right about AI that no one else gets right. But one thing that I'd like to see less of is that there's a particular model I often see from -- not all but some -- some people in EA which is sort of that there's like two progress bars and one of them is like the AI capabilities progress bar and the other is the AI safety progress bar and if the AI capabilities progress bar reaches the end before the AI safety progress bar then we all die, and if the AI safety progress bar reaches the end first then it's great. I think this model is kind of dangerous because I think it's inaccurate and it drives the impression among AI researchers that AI safety people think their work is evil or trying to hold back their work and I think that does push against acceptance. The reason I think it's not right is that at least in my own experience in a lot of the safety work I do is made possible by research and advances in capabilities work. A lot of the safety work I do also allows you to do capabilities work in different ways that's maybe safety-compatible. I also think that relating to AGI in particular some of the safety work that we end up doing with respect to AGI is a lot of it may only be possible by within the last two or three years before AGI. So there's a lot of entwinement between these two things and while it's true that I think that we should work on safety research right away -- we shouldn't wait until you know build AGI tomorrow and that's why I'm working on it now -- you know I think we should maybe be less extreme and chill a little bit about you know being scared about the next breakthrough in capabilities. It's possible that we will have a hard time doing all the safety research in the end but ther'es a good amount that we may be able to do . The situation is just not as binary and i think that frame polarizes the AI researchers against AI safety researchers and I think that's unhelpful.

# See also

- [List of discussions between Paul Christiano and Wei Dai]()
