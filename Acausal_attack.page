---
title: Acausal attack
format: markdown
categories: AI_safety
...

There seems to be an idea or a group of similar ideas regarding AGI being attacked due to simulating distant superintelligences. Some terms I've seen used to describe this:

- [malign prior](https://www.google.com/search?q=%22malign%20prior%22)
- [acausal attack](https://www.google.com/search?q=%22acausal%20attack%22)
- [probable environment hacking](https://arbital.com/p/probable_environment_hacking/), "coercing the most probable environment of your AI", etc. (see also comments on that page)
- [weirdness of the universal prior](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/)
