---
title: AI safety
format: markdown
categories: AI_safety Cause_areas existential-risks
...

**Note**: As of late 2017/2018, Issa is working on [AI Watch](https://aiwatch.issarice.com/), which is partly motivated by cause prioritization.

# Summary

Importance
:    \<importance rating\>

Tractability
:    \<tractability rating\>

Neglectedness
:    \<neglectedness rating\>


<!-- keywords: ai siai miri singularity eliezer yudkowsky agi
               asi fai friendly ai
-->

[AI Impacts](http://www.aiimpacts.org/) is an informational site that "aims to improve our understanding of the likely impacts of human-level artificial intelligence".

The main EA organization working in this field seems to be [MIRI](http://intelligence.org/), which does relevant math research and sponsors forecasting projects like [AI Impacts](http://www.aiimpacts.org/).
FHI might also have more information.

[*Superintelligence: Paths, Dangers, Strategies*](http://www.amazon.com/dp/0199678111/ref=cm_sw_su_dp) by [Nick Bostrom](http://www.nickbostrom.com/) lays out a foundation for navigating scenarios where machine brains surpass human brains in general intelligence.

[Artificial General Intelligence](http://www.scholarpedia.org/article/Artificial_General_Intelligence)

[How large do you think the first Strong AI be\(lines of code, servers etc\)? • /r/artificial](https://www.reddit.com/r/artificial/comments/2qyg8j/how_large_do_you_think_the_first_strong_ai/cnathbn)

[Artificial Intelligence • /r/artificial](https://www.reddit.com/r/artificial/)

[Comparative Table of Cognitive Architectures \(started on October 27, 2009; last update: June 18, 2012\)](http://bicasociety.org/cogarch/architectures.htm)

# Importance

FIXME

# Tractability

FIXME

# Neglectedness

FIXME

# See also

- [Views on AI safety]()
- [Simulation hypothesis]()

# External links

- Some relevant timelines:
    - [Timeline of AI safety](https://timelines.issarice.com/wiki/Timeline_of_AI_safety)
    - [Timeline of Machine Intelligence Research Institute](https://timelines.issarice.com/wiki/Timeline_of_Machine_Intelligence_Research_Institute)
    - [Timeline of Center for Applied Rationality](https://timelines.issarice.com/wiki/Timeline_of_Center_for_Applied_Rationality)
    - [Timeline of Berkeley Existential Risk Initiative](https://timelines.issarice.com/wiki/Timeline_of_Berkeley_Existential_Risk_Initiative)
    - [Timeline of Future of Humanity Institute](https://timelines.issarice.com/wiki/Timeline_of_Future_of_Humanity_Institute)
    - [Timeline of Foundational Research Institute](https://timelines.issarice.com/wiki/Timeline_of_Foundational_Research_Institute)
    - [Timeline of OpenAI](https://timelines.issarice.com/wiki/Timeline_of_OpenAI)
- [CarlShulman comments on How does MIRI Know it Has a Medium Probability of Success?](http://lesswrong.com/lw/i7p/how_does_miri_know_it_has_a_medium_probability_of/9i5b)
- [Don’t Worry, Smart Machines Will Take Us With Them](http://nautil.us/issue/28/2050/dont-worry-smart-machines-will-take-us-with-them)
-   Jeff Kaufman's posts on AI safety:
    -   ["Looking into AI Risk"](http://www.jefftk.com/p/looking-into-ai-risk)
    -   ["Superintelligence Risk Project"](http://www.jefftk.com/p/superintelligence-risk-project)
    -   ["Conversation with Dario Amodei"](http://www.jefftk.com/p/conversation-with-dario-amodei)
    -   ["Conversation with Michael Littman"](http://www.jefftk.com/p/conversation-with-michael-littman)
    -   ["Superintelligence Risk Project Update"](http://www.jefftk.com/p/superintelligence-risk-project-update)
